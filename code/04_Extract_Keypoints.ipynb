{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://imgur.com/1ZcRyrc.png' style='float: left; margin: 20px; height: 55px'>\n",
    "\n",
    "\n",
    "# DSI-SG-42 Capstone Project:\n",
    "### Silent Echoes: From Hand Waves to Written Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extract Keypoints With MediaPipe\n",
    "\n",
    "In this notebook, we will just be extracting the keypoints (coordinates) using MediaPipe library and exporting it each video (60 frames) into an empty folder. This will allow more efficient compiling for modeling in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging to write to a file\n",
    "log_dir = ('../log_files')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "log_file = os.path.join(log_dir, 'extract_keypoints.log')\n",
    "\n",
    "# Setup Logger\n",
    "logger = logging.getLogger()  # Get the root logger\n",
    "for handler in logger.handlers:  # Remove all old handlers\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format = '%(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# set display settings\n",
    "%matplotlib inline\n",
    "pd.set_option('display.width', 100000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We intend to build 3 models with varying complexity. The first model would be the most complex, taking in landmark values from the face, pose, left, and right hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# These are the actions that we intend to train and predict\n",
    "\n",
    "# the first 3 words that we will be modeling upon\n",
    "first_three = ['please', 'sorry', 'hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Extracting keypoints with OpenCV and MediaPipe\n",
    "\n",
    "\n",
    "We will extract the keypoints from each video file and save it as a numpy array file in the respective folders created earlier. We would also like to view how the landmarks are superimposed on the video to see how it tracks the signer's movement.\n",
    "\n",
    "We will be intending to build 3 models:\n",
    "- the first model will be extracting face, pose, left, and right hand landmarks and will be known as the `Comprehensive Model`\n",
    "- the second model will be extracting pose, left and right hand landmarks and will be known as the `PH Model`\n",
    "- the third model will be extracting left and right hand landmarks and will be known as the `Hands Model`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with keypoint extraction with MediaPipe and OpenCV library, we will need to set two lines of code to allow us to initiate tools to track body positions.\n",
    "- The `mp.solutions.holistic` is a pre-trained model from MediaPipe library for body pose estimation which will be vital in tracking the hand movements of the video.\n",
    "- The `mp.solutions.drawing_utils` allows the ability to draw landmarks, connections, and annotations in images and videos. They will be extremely useful to visualize the landmarks and connections of how it tracks the movement of the hand movements of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Assign and initiate MediaPipe tools\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utililties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to work as intended, our values of our videos are in Red-Green-Blue but for our model to perform a pose estimation, the colours will need to be converted to Blue-Green-Red due to its programmed nature. Here we create a function that changes the video colours from RGB to BGR and let the model process that image and returns us back to RGB format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# to perform pose estimation\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Color conversion BGR 2 RGB\n",
    "    image.flags.writeable = False # Image is no longer writeable\n",
    "    results = model.process(image) # make predictions\n",
    "    image.flags.writeable = True # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # Color conversion RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to observe how the model tracks the body movement. We will create a function to draw the respective landamrks from the face, body pose, and both hands. \n",
    "\n",
    "Depending on the type of model complexity, a conditional statement is placed in this function and will need to be specified during the function call to either inspect the video clip or extract the datapoints out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# visualize the drawing of landmarks and connections on the body\n",
    "def draw_landmarks(image, results, model_type):\n",
    "\n",
    "    '''\n",
    "    As we will be conducting 3 models in varying complexity, we will include a conditional if statement where we can \n",
    "    assign which model_type (model complexity) we will be extracting the landmark values on\n",
    "\n",
    "    The first argument 'image' takes in the image\n",
    "    The second argument 'results.POSITIONAL_landmarks' refers to the type of landmarks to be used\n",
    "    The third argument 'mp_holistic.POSTIONAL_connections/contours' refers to the connecting joints for which POSITIONAL landmark\n",
    "    The fourth argument 'mp_drawing.DrawingSpec' draws the node/points on the body part\n",
    "    The fifth arguement 'mp_drawing.DrawingSpec' draws the connecting lines between adjacent landmarks/coordinates\n",
    "    '''\n",
    "\n",
    "    # comprehensive model\n",
    "    if model_type == 'full':\n",
    "        # Draw face connections\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                                mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                                mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                ) \n",
    "        # Draw pose connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw left hand connections\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw right hand connections  \n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        \n",
    "    # PH Model\n",
    "    elif model_type == 'ph':\n",
    "        \n",
    "        # Draw pose connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw left hand connections\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw right hand connections  \n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "\n",
    "    # Hands Model \n",
    "    elif model_type == 'hands':\n",
    "        \n",
    "        # Draw left hand connections\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw right hand connections  \n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "    \n",
    "    # if none selected print the following\n",
    "    else:\n",
    "        print('Please choose \"full\", \"ph\", or \"hands\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be conducting 3 models in varying complexity, we will include a conditional if statement where we can \n",
    "assign which model_type (model complexity) we will be extracting the landmark values on.\n",
    "\n",
    "Each of these groupings will be extracted and flatten iinto an array. However, there will be instances where the \n",
    "landmark or body part is not visible from the camera and we will assign a value of zeroes to ensure that the \n",
    "array shape is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# to extract the landmark values, x,y,z, visibility(pose) of the video\n",
    "def extract_keypoints(results, model_type):\n",
    "    '''\n",
    "    As we will be conducting 3 models in varying complexity, we will extract the landmark values according to the model specified\n",
    "    Each of the conditional statements have an object that extracts the landmark according to their coordinates. All landmarks have\n",
    "    x,y,z values except for 'pose' which has an added value termed 'visibility'.\n",
    "    \n",
    "    The origin (0,0,0) is on the top-left of the frame. The greater the value the landmark is lower in the frame. Conversely, \n",
    "    The smaller the value the the landmark is higher in the frame. This will apply for both x and y values. \n",
    "    The z-value measures the distance/depth from the camera with greater value indicating the point further from the camera\n",
    "    The visibility value measures the confidence that the landmark is in frame with a higher value indicating that the landmark \n",
    "    is present and are accurately detected\n",
    "\n",
    "\n",
    "    The x-value is the horizontal coordinate of the frame, \n",
    "        y-value is the vertical coordinate of the frame, \n",
    "        z-value is the distance coordinate to the camera in the frame\n",
    "        visibility is the confidence score of the landmark being detected\n",
    "\n",
    "    A conditional statement is established such that if the landmark is not detected, fill the row with zeros to ensure correct shape will be output.\n",
    "    Each landmark has different number of coordinates and we will multiply the number of landmarks by the number of coordinates present in each landmark.\n",
    "    '''\n",
    "\n",
    "    # Comprehensive Model\n",
    "    if model_type == 'full':\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "        return np.concatenate([face, pose, lh, rh])\n",
    "    \n",
    "    # PH Model\n",
    "    elif model_type == 'ph':\n",
    "\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "        return np.concatenate([pose, lh, rh])\n",
    "    \n",
    "    # Hands Model\n",
    "    elif model_type == 'hands':\n",
    "        \n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "        return np.concatenate([lh, rh])\n",
    "    \n",
    "    # if none chosen, print the following\n",
    "    else:\n",
    "        print('Please choose \"full\", \"ph\", or \"hands\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function to loop through each actions in the respective folder. In this function will need to specifywhich video dataset we are using - `train_data`, `test_data`, and `val_data` - and a decide a folder name that the files will be created in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# custom function to loop through each words and save extracted keypoints in numpy array\n",
    "def process_dataset_words(test_train_val_data: str, extracted_keypoints_folder: str, model_type: str):\n",
    "    '''\n",
    "    This function will loop through each of the train/test/val folder in the directory, extracting the relevant word\n",
    "    class in the respective folder - 'please', 'sorry', 'hello'. The last argument in the function will take in\n",
    "    what landmarks to be extracted based on the model type\n",
    "    \n",
    "    This function will also call on functions 'draw_landmarks' and 'extract_keypoints' that was created earlier.\n",
    "    During the extraction of landmarks, we can inspect what MediaPipe detects in each of the video.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # iterate through the first three words\n",
    "    for word in first_three:\n",
    "        # get the path to the folder containing videos for the current word\n",
    "        videos_path = os.path.join('../videos', test_train_val_data, 'output', word)\n",
    "        videos = os.listdir(videos_path)  # Get the list of videos for the current word\n",
    "\n",
    "        # Check if there are any videos to process\n",
    "        if videos:\n",
    "            # iterate through each video\n",
    "            for count, vid in enumerate(videos):\n",
    "                # create the path for storing extracted keypoints\n",
    "                video_folder_path = os.path.join('../data', extracted_keypoints_folder, word, str(count))\n",
    "\n",
    "                # check if the directory exists, if not create one\n",
    "                if not os.path.exists(video_folder_path):\n",
    "                    os.makedirs(video_folder_path, exist_ok=True)\n",
    "                    # print(f\"Creating directory: {video_folder_path}\")  # debug\n",
    "                # else:                                                  # debug\n",
    "                #     print(f\"Directory already exists: {video_folder_path}\")  # debug\n",
    "\n",
    "                # construct video path    \n",
    "                VIDEO_PATH = os.path.join(videos_path, vid)\n",
    "\n",
    "                # read the video path\n",
    "                cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                max_frames = min(frame_count, 60)\n",
    "\n",
    "                # initialize MediaPipe Holistic Model\n",
    "                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                    frames_processed = 0\n",
    "\n",
    "                    # process each frame in the video\n",
    "                    while frames_processed < max_frames:\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "\n",
    "                        # perform detection using MediaPipe\n",
    "                        image, results = mediapipe_detection(frame, holistic)\n",
    "                        draw_landmarks(image, results, model_type)\n",
    "                        keypoints = extract_keypoints(results, model_type)\n",
    "\n",
    "                        # create npy file and write keypoint value inside\n",
    "                        npy_path = os.path.join(video_folder_path, str(frames_processed) + '.npy')\n",
    "                        np.save(npy_path, keypoints)\n",
    "\n",
    "                        # display the image with landmarks\n",
    "                        cv2.imshow(test_train_val_data, image)\n",
    "\n",
    "                        # break gracefully\n",
    "                        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                        frames_processed += 1\n",
    "\n",
    "                    # Pad remaining frames\n",
    "                    while frames_processed < 60:\n",
    "                        npy_path = os.path.join(video_folder_path, str(frames_processed) + '.npy')\n",
    "                        keypoints_pad = np.zeros_like(keypoints)\n",
    "                        np.save(npy_path, keypoints_pad)\n",
    "                        frames_processed += 1\n",
    "                        \n",
    "                # Release video capture object and close all OpenCV windows\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "        else:\n",
    "            print(f\"No videos found for {word}. No directory created.\")\n",
    "\n",
    "    return 'All Videos Processed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be collecting the landmark values in a numpy array. Here, we will create a new folder `training_keypoints` in the `data` folder which the datapoints can be saved as a .npy file. Each numbered folder in the list represents one video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to create a folder for each video to store landmark values for each word action\n",
    "\n",
    "def make_numpy_directory(test_train_val_data: str, extracted_keypoints_folder:str):\n",
    "    \n",
    "    '''\n",
    "    This functions serves to create a folder for each word class in the data folder \n",
    "    to allow the numpy arrays to be stored\n",
    "    '''\n",
    "\n",
    "\n",
    "    # create directory and ensure that directory is present\n",
    "    os.makedirs('../data/' + extracted_keypoints_folder + '/', exist_ok=True) \n",
    "\n",
    "    # Loop for each signed word in the list of words\n",
    "    for word in first_three:\n",
    "        count = 0  # Start numbering video files from 1\n",
    "\n",
    "        # Iterate each video file in the signed word folder\n",
    "        video_directory = '../videos/' + test_train_val_data + '/' + word + '/cropped_videos/'\n",
    "        videos = os.listdir(video_directory)\n",
    "        if videos:  # Check if there are any videos to process\n",
    "            for vid in videos:\n",
    "                # Create a new folder to store the numpy array values\n",
    "                numpy_folder_path = os.path.join('../data', extracted_keypoints_folder, word, str(count))\n",
    "                os.makedirs(numpy_folder_path, exist_ok=True)\n",
    "                # print(f\"Creating or verifying directory: {numpy_folder_path}\")  # Debug: log directory creation\n",
    "                count += 1  # Increment the counter for the next video\n",
    "        else:\n",
    "            print(f\"No videos found in {video_directory}\")  # Inform if no videos found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Extraction of Face, Pose, Left, and Right Hand Keypoints\n",
    "\n",
    "\n",
    "This will be the most comprehensive dataset of all the models as it will contain information on the face, pose, and hands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# create folders to store the data from the extraction process\n",
    "\n",
    "# create folders for train,test, and val data\n",
    "make_numpy_directory('train_data', 'training_full_keypoints')\n",
    "make_numpy_directory('test_data', 'testing_full_keypoints')\n",
    "make_numpy_directory('val_data', 'val_full_keypoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 8min 20s\n",
      "Wall time: 58min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All Videos Processed'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extract datapoints for the comprehensive model \n",
    "\n",
    "# training model\n",
    "process_dataset_words('train_data', 'training_full_keypoints', 'full')\n",
    "\n",
    "# # testing model\n",
    "process_dataset_words('test_data', 'testing_full_keypoints', 'full')\n",
    "\n",
    "# val model\n",
    "process_dataset_words('val_data', 'val_full_keypoints', 'full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Extraction of Pose, Left, and Right Hand Keypoints\n",
    "\n",
    "\n",
    "We will no reduce our model's complexity and remove facial landmarks and only be extracting datapoints from the pose, left and right hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# create folders to store the data from the extration process\n",
    "\n",
    "# create folders for train,test, and val data\n",
    "make_numpy_directory('train_data', 'training_ph_keypoints')\n",
    "make_numpy_directory('test_data', 'testing_ph_keypoints')\n",
    "make_numpy_directory('val_data', 'val_ph_keypoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 2min 33s\n",
      "Wall time: 53min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All Videos Processed'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extract datapoints for the model that only has datapoints from the pose, left and right hands\n",
    "\n",
    "# training model\n",
    "process_dataset_words('train_data', 'training_ph_keypoints', 'ph')\n",
    "\n",
    "# testing model\n",
    "process_dataset_words('test_data', 'testing_ph_keypoints', 'ph')\n",
    "\n",
    "# val model\n",
    "process_dataset_words('val_data', 'val_ph_keypoints', 'ph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Extraction of Left and Right Hand Keypoints\n",
    "\n",
    "\n",
    "The last model we will be building is the simplest model which will only contain the datapoints from both hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# create folders to store the data from the extration process\n",
    "\n",
    "# create folders for train,test, and val data\n",
    "make_numpy_directory('train_data', 'training_hands_keypoints')\n",
    "make_numpy_directory('test_data', 'testing_hands_keypoints')\n",
    "make_numpy_directory('val_data', 'val_hands_keypoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 2min 18s\n",
      "Wall time: 52min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All Videos Processed'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extract datapoints for the the model that only has hand landmarks\n",
    "\n",
    "# training model\n",
    "process_dataset_words('train_data', 'training_hands_keypoints', 'hands')\n",
    "\n",
    "# testing model\n",
    "process_dataset_words('test_data', 'testing_hands_keypoints', 'hands')\n",
    "\n",
    "# val model\n",
    "process_dataset_words('val_data', 'val_hands_keypoints', 'hands')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
